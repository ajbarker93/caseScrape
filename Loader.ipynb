{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of HUDOC documents\n",
    "# Corpus from 31/01/2019, scraped using https://github.com/ajbarker93/caseScrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from price_parser import Price\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from forex_python.converter import CurrencyRates\n",
    "import seaborn as sns\n",
    "import datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check drive contents\n",
    "rootdir=\"/users/adambarker/case_scrape/\"\n",
    "os.chdir(rootdir)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle file of results content\n",
    "results = pd.read_pickle(\"./results0.pkl\")\n",
    "\n",
    "# Remove all rows where the content failed\n",
    "results = results.loc[results['content']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append violations\n",
    "\n",
    "reload = 1\n",
    "if reload:\n",
    "    # Add additional fields\n",
    "    results['violation'] = ''\n",
    "    results['conclusion'] = ''\n",
    "    results['importance'] = ''\n",
    "    results['originatingbody'] = '' \n",
    "    results['case_date'] = ''\n",
    "\n",
    "    # Load summaries\n",
    "    onlyfiles = [f for f in listdir(rootdir+\"/summaries/\") if isfile(join(rootdir+\"/summaries/\", f))]\n",
    "    onlyjsons = [f for f in onlyfiles if f.endswith('.json')]\n",
    "\n",
    "    # Examine summaries and choose the list of P1-1 relevant cases\n",
    "    for onlyjson in onlyjsons:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            with open(rootdir+\"/summaries/\"+onlyjson) as json_file: # first open the batches of summaries\n",
    "\n",
    "                contents = json.load(json_file)\n",
    "                contents2 = contents['results']\n",
    "\n",
    "                for i in range(0,len(contents2)): # loop through each case in the batch\n",
    "\n",
    "                    field_data = contents2[i]['columns']\n",
    "\n",
    "                    if field_data['itemid'] in results.case_id.values:\n",
    "\n",
    "                        results.loc[results['case_id'] == field_data['itemid'],['violation']] = field_data['violation']\n",
    "                        results.loc[results['case_id'] == field_data['itemid'],['conclusion']] = field_data['conclusion']\n",
    "                        results.loc[results['case_id'] == field_data['itemid'],['importance']] = field_data['importance']\n",
    "                        results.loc[results['case_id'] == field_data['itemid'],['originatingbody']] = field_data['originatingbody']\n",
    "\n",
    "                        \n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            \n",
    "    # Save as pickle again for easy recall\n",
    "    results.to_pickle(\"./results.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some content\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a loading script which does all of the io on the directory. It contains:\n",
    "# Strings to define start and end of relevant sections\n",
    "\n",
    "reload = 1\n",
    "if reload:\n",
    "\n",
    "    # Define a simple set of start and end wildcards for the claimed and awarded sections\n",
    "    cl_start_str='SECTION'\n",
    "    cl_end_str='President'\n",
    "    aw_start_str='SECTION'\n",
    "    aw_end_str='President'\n",
    "\n",
    "    # First do English cases\n",
    "    for i in range(0,len(results)):\n",
    "\n",
    "        result = results['content'].iloc[i]\n",
    "\n",
    "        if len(result.split(cl_start_str))>1:\n",
    "\n",
    "            # Choose text from start wildcard onwards\n",
    "            to_end = result.split(cl_start_str)[1]\n",
    "            # Choose text before end wildcard\n",
    "            from_start = to_end.split(cl_end_str)[0]\n",
    "            # Add to resuls array\n",
    "            results['claimed'].iloc[i] = from_start\n",
    "\n",
    "        if len(result.split(aw_start_str))>1:\n",
    "\n",
    "            # Repeat for awarded section\n",
    "            to_end = result.split(aw_start_str)[1]\n",
    "            from_start = to_end.split(aw_end_str)[0]\n",
    "            results['awarded'].iloc[i] = from_start\n",
    "                \n",
    "    # Then do French cases\n",
    "    cl_start_str='Note d’information'\n",
    "    cl_end_str='Cliquez ici pour accéder aux'\n",
    "    aw_start_str='Note d’information'\n",
    "    aw_end_str='Cliquez ici pour accéder aux'\n",
    "\n",
    "    for i in range(0,len(results)):\n",
    "\n",
    "        if results['claimed'].iloc[i] =='':\n",
    "\n",
    "            result = results['content'].iloc[i]\n",
    "\n",
    "            if len(result.split(cl_start_str))>1:\n",
    "\n",
    "                # Choose text from start wildcard onwards\n",
    "                to_end = result.split(cl_start_str)[1]\n",
    "                # Choose text before end wildcard\n",
    "                from_start = to_end.split(cl_end_str)[0]\n",
    "                # Add to resuls array\n",
    "                results['claimed'].iloc[i] = from_start\n",
    "\n",
    "            if len(result.split(aw_start_str))>1:\n",
    "\n",
    "                # Repeat for awarded section\n",
    "                to_end = result.split(aw_start_str)[1]\n",
    "                from_start = to_end.split(aw_end_str)[0]\n",
    "                results['awarded'].iloc[i] = from_start         \n",
    "            \n",
    "    # Save as pickle again for easy recall\n",
    "    results.to_pickle(\"./results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get case dates\n",
    "\n",
    "try: \n",
    "    for i in range(0,len(results)):\n",
    "\n",
    "        # Get dates while we're here look at first 40 words\n",
    "        headline = result.split(\"JUDGMENT\")\n",
    "        if len(headline)>1:\n",
    "\n",
    "            specific_split = headline[1].split()\n",
    "            split_depth = min(20,len(specific_split))\n",
    "            date_str = ' '.join(specific_split[:split_depth])\n",
    "            matches = list(datefinder.find_dates(date_str))\n",
    "\n",
    "            if len(matches) > 1:\n",
    "                # date returned will be a datetime.datetime object. here we are only using the first match.\n",
    "                results['case_date'].iloc[i] = matches[1]   \n",
    "            elif len(matches)==1:\n",
    "                results['case_date'].iloc[i] = matches[0]\n",
    "\n",
    "\n",
    "        # Get dates while we're here look at first 40 words\n",
    "        headline = result.split(\"Arrêt\")\n",
    "        if len(headline)>1:\n",
    "            specific_split = headline[1].split()\n",
    "            split_depth = min(20,len(specific_split))\n",
    "            date_str = ' '.join(specific_split[:split_depth])\n",
    "            matches = list(datefinder.find_dates(date_str))\n",
    "            if len(matches) > 1:\n",
    "                # date returned will be a datetime.datetime object. here we are only using the first match.\n",
    "                results['case_date'].iloc[i] = matches[1]   \n",
    "            elif len(matches)==1:\n",
    "                results['case_date'].iloc[i] = matches[0]  \n",
    "                \n",
    "except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results['case_date'] = ''\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "# Load pickle if required\n",
    "reload = 0\n",
    "if reload:\n",
    "    results = pd.read_pickle(\"./results.pkl\") \n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find numerical content and try to arrive at a claimed figure\n",
    "\n",
    "# Define list of allowed currencies\n",
    "curr = ['EUR','EURO','GBP','USD','Euros','Pounds','Dollars','French','Turkish','FRF','Lira','Turkish Lira']\n",
    "\n",
    "# Remove all rows where the content failed\n",
    "results = results.loc[results['claimed']!='']\n",
    "results = results.loc[results['awarded']!='']\n",
    "\n",
    "# Define function to find currencies in string\n",
    "def find_currency(x):\n",
    "    px = [Price.fromstring(i) for i in x.split(\".\")]\n",
    "    px2 = [t.amount for t in px if t.currency in curr]\n",
    "    return px2\n",
    "\n",
    "# Update the num claimed col\n",
    "results['num_claimed'] = results.apply(lambda x: find_currency(str(x['claimed'])),axis=1)\n",
    "results['num_awarded'] = results.apply(lambda x: find_currency(str(x['awarded'])),axis=1)\n",
    "\n",
    "# Set the fields to zero if they contain an empty list\n",
    "results.num_awarded = results.num_awarded.apply(lambda y: [0] if (len(y)==0) else y)\n",
    "results.num_claimed = results.num_claimed.apply(lambda y: [0] if (len(y)==0) else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where the content failed\n",
    "results = results.loc[results['violation']!='']\n",
    "\n",
    "# Inspect results\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wildcards for selecting P1-1 cases\n",
    "wildcards_p1 = ['1-1','P1','P1-1','P1-1-1','Protocol 1-1','Article 1','Article 1 al. 1 du Protocole','1 du Protocole n° 1','1-2']\n",
    "wildcards_p2 = ['P2','2-1']\n",
    "wildcards_p3 = ['P3']\n",
    "wildcards_p4 = ['P4','4-2']\n",
    "wildcards_p5 = ['P5','5-2','5-3','5-4','5-1','5-5']\n",
    "wildcards_p6 = ['P6','6-1','6-3','6-2']\n",
    "wildcards_p8 = ['P8','8-1']\n",
    "wildcards_p13 = ['13-P4','13','13+3','13+7','13+5']\n",
    "\n",
    "# Do filtering into various articles\n",
    "results['article'] = ''\n",
    "results['article'] = results.apply(lambda x: 'p1' if any(i in x['violation'] for i in wildcards_p1) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p2' if any(i in x['violation'] for i in wildcards_p2) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p4' if any(i in x['violation'] for i in wildcards_p4) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p5' if any(i in x['violation'] for i in wildcards_p5) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p6' if any(i in x['violation'] for i in wildcards_p6) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p8' if any(i in x['violation'] for i in wildcards_p8) else x['article'],axis=1)\n",
    "results['article'] = results.apply(lambda x: 'p13' if any(i in x['violation'] for i in wildcards_p13) else x['article'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define originator\n",
    "results['originator'] = ''\n",
    "results['defendant'] = ''\n",
    "\n",
    "try:\n",
    "    # Split to get originator and defendant\n",
    "    # Source\n",
    "    results['originator'] = results.apply(lambda x: x['case'].split(\" v.\")[0].split(\" \")[-1] if len(x['case'].split(\" v.\"))>1 else x['originator'],axis=1)\n",
    "    results['originator'] = results.apply(lambda x: x['case'].split(\" c.\")[0].split(\" \")[-1] if len(x['case'].split(\" c.\"))>1 else x['originator'],axis=1)\n",
    "    # Defendant\n",
    "    results['defendant'] = results.apply(lambda x: x['case'].split(\" v. \")[1].split(\" \")[0] if len(x['case'].split(\" v. \"))>1 else x['defendant'],axis=1)\n",
    "    results['defendant'] = results.apply(lambda x: x['case'].split(\" c. \")[1].split(\" \")[0] if len(x['case'].split(\" c. \"))>1 else x['defendant'],axis=1)\n",
    "\n",
    "    # Title format the originator and defendant\n",
    "    results['originator'] = results.apply(lambda x: x['originator'].title(),axis=1)\n",
    "    results['defendant'] = results.apply(lambda x: x['defendant'].title(),axis=1)\n",
    "\n",
    "    # Add the first name\n",
    "    results['originator'] = results.apply(lambda x: x['content'].split(str(x['originator']))[0].split(\" \")[-1] + \" \" + x['originator'] if len(x['originator'])>1 else x['originator'],axis=1)\n",
    "    \n",
    "    # Then look in text for prefix or suffix\n",
    "    results['type'] = ''\n",
    "    results['type'] = results.apply(lambda x: 'M' if 'Mr '+x['originator'] in x['content'] else x['type'],axis=1)\n",
    "    results['type'] = results.apply(lambda x: 'F' if 'Mrs '+x['originator'] in x['content'] else x['type'],axis=1)\n",
    "\n",
    "except:\n",
    "    print(\"Error\")\n",
    "\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle again for easy recall\n",
    "results.to_pickle(\"./results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
